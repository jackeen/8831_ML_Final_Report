\section{Final discuss and Recommendation}

\textbf{Random Forest} achieved 85.4\% accurate rate on the IMDb test set. As a traditional machine learning model, it relies on feature engineering (e.g., TF-IDF vectors) and decision tree ensembles. Its strengths include computational efficiency, minimal hardware requirements (trainable on CPUs), and interpretability. However, its accuracy lags means that it unable to capture complex semantic relationships in the sequence, which limits its performance compared to neural network models.

\textbf{LSTM} recorded an accuracy of 85.5\%, it is slightly outperforming Random Forest, reflecting better handling of sequential data. As a recurrent neural network, it excels at modeling the sequential nature of text, which makes it suitable for capturing the flow of sentiments in reviews. It requires moderate computational resources, and less demanding than transformer models, offering a balance between performance and efficiency. However, its accuracy is limited by its unidirectional processing and struggling with long dependencies, this feature lead to lower performance compared to more advanced architectures.

\textbf{BERT} achieved an outstanding 94.0\% accuracy on the test set, with an evaluation loss of 0.2147, it demonstrates strong generalization. Its ROC curve is expected to approach the top-left corner and an AUC is 0.98, which indicates excellent ability to distinguish positive and negative sentiments. BERT's transformer architecture is pre-trained to handle vast text, and it is fine-tuned on IMDb for capturing bidirectional context and complex linguistic patterns. Its major downside is computational intensity, the experiment requires significant resources depend on the start of BERT methodology. Moreover, its complex architecture reduces interpretability compared to Random Forest.

BERT's 94.0\% accuracy significantly surpasses Random Forest and LSTM. The result highlights the superiority of transformers in capturing text semantics. For future work, to explore lighter transformers like DistilBERT for efficiency. If resources are limited, the Random Forest with word embeddings or opt for LSTM is suitable. For top performance, BERT remains the best choice.